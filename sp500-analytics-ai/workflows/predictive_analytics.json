{
  {
  "Defaults": {
    "Host": "replace-with-usercode_instruqt_server",
    "CreatedBy": "replace-with-email",
    "ControlmServer": "IN01",
    "Application": "replace-with-usercode_SAP500",
    "SubApplication": "replace-with-usercode_SAP500",
    "OrderMethod": "Manual"
  },
  "replace-with-usercode-SP-500-vertex-AI" : {
    "Type" : "Folder",
    "ActiveRetentionPolicy" : "CleanEndedOK",
    "When" : {
      "RuleBasedCalendars" : {
        "Included" : [ "EVERYDAY" ],
        "EVERYDAY" : {
          "Type" : "Calendar:RuleBased",
          "When" : {
            "DaysRelation" : "OR",
            "WeekDays" : [ "NONE" ],
            "MonthDays" : [ "ALL" ]
          }
        }
      }
    },
    "BigQuery-Job" : {
      "Type" : "Job:GCP BigQuery",
      "ConnectionProfile" : "replace-with-usercode-uc-BQ-CP",
      "Project Name" : "sso-gcp-dba-ctm4-pub-cc10274",
      "Dataset Name" : "sp500_dataset",
      "Action" : "Query",
      "Run Select Query and Copy to Table" : "unchecked",
      "SQL Statement" : "CREATE OR REPLACE TABLE `sso-gcp-dba-ctm4-pub-cc10274.sp500_dataset.sp500_features` AS%4ESELECT%4EDATE(date) AS date,%4Eopen,%4Ehigh,%4Elow,%4Eclose,%4Evolume,%4EName,%4ESAFE_DIVIDE(close - LAG(close) OVER (PARTITION BY Name ORDER BY DATE(date)),LAG(close) OVER (PARTITION BY Name ORDER BY DATE(date))) AS daily_return,%4EAVG(close) OVER (PARTITION BY Name ORDER BY DATE(date) ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS ma_7,%4EAVG(close) OVER (PARTITION BY Name ORDER BY DATE(date) ROWS BETWEEN 29 PRECEDING AND CURRENT ROW) AS ma_30,%4ESTDDEV(close) OVER (PARTITION BY Name ORDER BY DATE(date) ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS volatility_7d,%4EAVG(close) OVER (PARTITION BY Name ORDER BY DATE(date) ROWS BETWEEN 13 PRECEDING AND CURRENT ROW) AS RSI_14,%4EAVG(close) OVER (PARTITION BY Name ORDER BY DATE(date) ROWS BETWEEN 11 PRECEDING AND CURRENT ROW)%4E-%4EAVG(close) OVER (PARTITION BY Name ORDER BY DATE(date) ROWS BETWEEN 25 PRECEDING AND CURRENT ROW) AS MACD,%4EAVG(close) OVER (PARTITION BY Name ORDER BY DATE(date) ROWS BETWEEN 8 PRECEDING AND CURRENT ROW) AS MACD_signal,%4EAVG(close) OVER (PARTITION BY Name ORDER BY DATE(date) ROWS BETWEEN 19 PRECEDING AND CURRENT ROW)%4E+%4E2 * STDDEV(close) OVER (PARTITION BY Name ORDER BY DATE(date) ROWS BETWEEN 19 PRECEDING AND CURRENT ROW) AS bb_upper,%4EAVG(close) OVER (PARTITION BY Name ORDER BY DATE(date) ROWS BETWEEN 19 PRECEDING AND CURRENT ROW) AS bb_middle,%4EAVG(close) OVER (PARTITION BY Name ORDER BY DATE(date) ROWS BETWEEN 19 PRECEDING AND CURRENT ROW)%4E-%4E2 * STDDEV(close) OVER (PARTITION BY Name ORDER BY DATE(date) ROWS BETWEEN 19 PRECEDING AND CURRENT ROW) AS bb_lower,%4EAVG(close) OVER (PARTITION BY Name ORDER BY DATE(date) ROWS BETWEEN 19 PRECEDING AND CURRENT ROW) AS EMA20%4EFROM `sso-gcp-dba-ctm4-pub-cc10274.sp500_dataset.sp500_clean`;%4E",
      "Copy Operation Type" : "Copy",
      "Show Load Options" : "unchecked",
      "Description" : "This job further proccessses the clean data and feature engineering",
      "RunAs" : "replace-with-usercode-uc-BQ-CP",
      "When" : {
        "WeekDays" : [ "NONE" ],
        "MonthDays" : [ "ALL" ],
        "DaysRelation" : "OR"
      },
      "eventsToAdd" : {
        "Type" : "AddEvents",
        "Events" : [ {
          "Event" : "GCP_BigQuery_Job_1-TO-GCP_Vertex_AI_Job_300"
        } ]
      }
    },
    "Looker-refresh-job" : {
      "Type" : "Job:GCP Functions",
      "ConnectionProfile" : "replace-with-usercode-uc-FUNCTIONS-CP",
      "Function Parameters" : "URL Parameters",
      "Get Logs" : "unchecked",
      "att2" : "\"filter\":\"labels.execution_id:",
      "att3" : "\"orderBy\": \"timestamp desc\",   \"pageSize\": 1000 }",
      "Project ID" : "sso-gcp-dba-ctm4-pub-cc10274",
      "Location" : "us-west2",
      "Function Name" : "looker-refresh",
      "Description" : "This job triggers the GCP function that refreshes the looker dashboard",
      "RunAs" : "replace-with-usercode-uc-FUNCTIONS-CP",
      "When" : {
        "WeekDays" : [ "NONE" ],
        "MonthDays" : [ "ALL" ],
        "DaysRelation" : "OR"
      },
      "eventsToWaitFor" : {
        "Type" : "WaitForEvents",
        "Events" : [ {
          "Event" : "GCP_Vertex_AI_Job_300-TO-Looker_refresh"
        } ]
      },
      "eventsToDelete" : {
        "Type" : "DeleteEvents",
        "Events" : [ {
          "Event" : "GCP_Vertex_AI_Job_300-TO-Looker_refresh"
        } ]
      }
    },
    "Vertex-AI-pipleine-Job" : {
      "Type" : "Job:GCP Vertex AI",
      "Pipeline Specification" : "{\n  \"components\": {\n    \"comp-load-preprocess-op\": {\n      \"executorLabel\": \"exec-load-preprocess-op\",\n      \"inputDefinitions\": {\n        \"parameters\": {\n          \"bucket_name\": {\n            \"parameterType\": \"STRING\"\n          },\n          \"project_id\": {\n            \"parameterType\": \"STRING\"\n          },\n          \"query\": {\n            \"parameterType\": \"STRING\"\n          }\n        }\n      },\n      \"outputDefinitions\": {\n        \"parameters\": {\n          \"Output\": {\n            \"parameterType\": \"STRING\"\n          }\n        }\n      }\n    },\n    \"comp-register-and-deploy-model-op\": {\n      \"executorLabel\": \"exec-register-and-deploy-model-op\",\n      \"inputDefinitions\": {\n        \"parameters\": {\n          \"bucket_name\": {\n            \"parameterType\": \"STRING\"\n          },\n          \"location\": {\n            \"parameterType\": \"STRING\"\n          },\n          \"model_path\": {\n            \"parameterType\": \"STRING\"\n          },\n          \"project_id\": {\n            \"parameterType\": \"STRING\"\n          }\n        }\n      },\n      \"outputDefinitions\": {\n        \"parameters\": {\n          \"Output\": {\n            \"parameterType\": \"STRING\"\n          }\n        }\n      }\n    },\n    \"comp-score-model-op\": {\n      \"executorLabel\": \"exec-score-model-op\",\n      \"inputDefinitions\": {\n        \"parameters\": {\n          \"bucket_name\": {\n            \"parameterType\": \"STRING\"\n          },\n          \"endpoint_name\": {\n            \"parameterType\": \"STRING\"\n          },\n          \"location\": {\n            \"parameterType\": \"STRING\"\n          },\n          \"project_id\": {\n            \"parameterType\": \"STRING\"\n          }\n        }\n      },\n      \"outputDefinitions\": {\n        \"parameters\": {\n          \"Output\": {\n            \"parameterType\": \"STRING\"\n          }\n        }\n      }\n    },\n    \"comp-train-model-op\": {\n      \"executorLabel\": \"exec-train-model-op\",\n      \"inputDefinitions\": {\n        \"parameters\": {\n          \"bucket_name\": {\n            \"parameterType\": \"STRING\"\n          },\n          \"processed_data_path\": {\n            \"parameterType\": \"STRING\"\n          }\n        }\n      },\n      \"outputDefinitions\": {\n        \"parameters\": {\n          \"Output\": {\n            \"parameterType\": \"STRING\"\n          }\n        }\n      }\n    }\n  },\n  \"deploymentSpec\": {\n    \"executors\": {\n      \"exec-load-preprocess-op\": {\n        \"container\": {\n          \"args\": [\n            \"--executor_input\",\n            \"{{$}}\",\n            \"--function_to_execute\",\n            \"load_preprocess_op\"\n          ],\n          \"command\": [\n            \"sh\",\n            \"-c\",\n            \"if ! [ -x \\\"$(command -v pip)\\\" ]; then    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pipfiPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'google-cloud-bigquery' 'google-cloud-storage' 'pandas' 'scikit-learn' 'joblib' 'db-dtypes'  &&  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.14.4' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\\\"3.9\\\"' && \\\"$0\\\" \\\"$@\\\"\",\n            \"sh\",\n            \"-ec\",\n            \"program_path=$(mktemp -d)printf \\\"%s\\\" \\\"$0\\\" > \\\"$program_path/ephemeral_component.py\\\"_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \\\"$program_path/ephemeral_component.py\\\"                         \\\"$@\\\"\",\n            \"import kfpfrom kfp import dslfrom kfp.dsl import *from typing import *def load_preprocess_op(project_id: str, query: str, bucket_name: str) -> str:    from google.cloud import bigquery, storage    import pandas as pd    from sklearn.model_selection import train_test_split    from sklearn.impute import SimpleImputer    from sklearn.preprocessing import StandardScaler    import joblib    import os    # Load data    bq = bigquery.Client(project=project_id)    df = bq.query(query).to_dataframe()    features = [        \\\"open\\\", \\\"high\\\", \\\"low\\\", \\\"close\\\", \\\"volume\\\", \\\"ma_7\\\", \\\"ma_30\\\",        \\\"volatility_7d\\\", \\\"RSI_14\\\", \\\"MACD\\\", \\\"MACD_signal\\\",        \\\"bb_upper\\\", \\\"bb_middle\\\", \\\"bb_lower\\\", \\\"EMA20\\\"    ]    target = \\\"daily_return\\\"    X = df[features]    y = df[target]    imputer = SimpleImputer(strategy=\\\"mean\\\")    X_imputed = imputer.fit_transform(X)    y_imputed = imputer.fit_transform(y.values.reshape(-1, 1)).flatten()    scaler = StandardScaler()    X_scaled = scaler.fit_transform(X_imputed)    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_imputed, test_size=0.2)    os.makedirs(\\\"/tmp/data\\\", exist_ok=True)    processed_path = \\\"/tmp/data/processed_data.joblib\\\"    joblib.dump(        {            \\\"X_train\\\": X_train,            \\\"X_test\\\": X_test,            \\\"y_train\\\": y_train,            \\\"y_test\\\": y_test,        },        processed_path,    )    storage_client = storage.Client(project=project_id)    bucket = storage_client.bucket(bucket_name)    blob = bucket.blob(\\\"pipeline_artifacts/processed_data.joblib\\\")    blob.upload_from_filename(processed_path)    return f\\\"gs://{bucket_name}/pipeline_artifacts/processed_data.joblib\\\"\"\n          ],\n          \"image\": \"python:3.9\"\n        }\n      },\n      \"exec-register-and-deploy-model-op\": {\n        \"container\": {\n          \"args\": [\n            \"--executor_input\",\n            \"{{$}}\",\n            \"--function_to_execute\",\n            \"register_and_deploy_model_op\"\n          ],\n          \"command\": [\n            \"sh\",\n            \"-c\",\n            \"if ! [ -x \\\"$(command -v pip)\\\" ]; then    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pipfiPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'google-cloud-aiplatform>=1.38.0'  &&  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.14.4' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\\\"3.9\\\"' && \\\"$0\\\" \\\"$@\\\"\",\n            \"sh\",\n            \"-ec\",\n            \"program_path=$(mktemp -d)printf \\\"%s\\\" \\\"$0\\\" > \\\"$program_path/ephemeral_component.py\\\"_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \\\"$program_path/ephemeral_component.py\\\"                         \\\"$@\\\"\",\n            \"import kfpfrom kfp import dslfrom kfp.dsl import *from typing import *def register_and_deploy_model_op(model_path: str, project_id: str, location: str, bucket_name: str) -> str:    from google.cloud import aiplatform as aiplatform    from google.cloud import bigquery, storage    # Initialize Vertex AI    aiplatform.init(project=project_id, location=location)    # Ensure the model is in GCS, upload it if needed    storage_client = storage.Client(project=project_id)    bucket = storage_client.bucket(bucket_name)    # Check if the model file exists in GCS, else upload it    local_model_path = \\\"/tmp/model/sp500_model.joblib\\\"    blob = bucket.blob(\\\"models/sp500_model.joblib\\\")    if not blob.exists():        print(\\\"Uploading model to GCS...\\\")        blob.upload_from_filename(local_model_path)    else:        print(\\\"Model already exists in GCS.\\\")    # Now use the GCS path for artifact_uri    model_uri = f\\\"gs://{bucket_name}/models/sp500_model.joblib\\\"    # Register the model in Vertex AI    model = aiplatform.Model.upload(        display_name=\\\"sp500-linear-regression-model-v1\\\",        artifact_uri=\\\"gs://sp500-etl/models/\\\",        serving_container_image_uri=\\\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-3:latest\\\",        description=\\\"Linear Regression model for predicting SP500 daily returns\\\"    )    print(f\\\"Model registered successfully. Model resource name: {model.resource_name}\\\")    # Deploy the model to an endpoint    endpoint = model.deploy(        deployed_model_display_name=\\\"sp500-endpoint\\\",        machine_type=\\\"n1-standard-2\\\"    )    print(f\\\"Model deployed successfully to endpoint: {endpoint.resource_name}\\\")    return endpoint.resource_name  # Return the endpoint resource name for use in the next step\"\n          ],\n          \"image\": \"python:3.10\"\n        }\n      },\n      \"exec-score-model-op\": {\n        \"container\": {\n          \"args\": [\n            \"--executor_input\",\n            \"{{$}}\",\n            \"--function_to_execute\",\n            \"score_model_op\"\n          ],\n          \"command\": [\n            \"sh\",\n            \"-c\",\n            \"if ! [ -x \\\"$(command -v pip)\\\" ]; then    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pipfiPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'google-cloud-bigquery' 'google-cloud-aiplatform' 'pandas' 'numpy' 'db-dtypes' 'pandas-gbq' 'scikit-learn'  &&  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.14.4' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\\\"3.9\\\"' && \\\"$0\\\" \\\"$@\\\"\",\n            \"sh\",\n            \"-ec\",\n            \"program_path=$(mktemp -d)printf \\\"%s\\\" \\\"$0\\\" > \\\"$program_path/ephemeral_component.py\\\"_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \\\"$program_path/ephemeral_component.py\\\"                         \\\"$@\\\"\",\n            \"import kfpfrom kfp import dslfrom kfp.dsl import *from typing import *def score_model_op(    endpoint_name: str,    project_id: str,    location: str,    bucket_name: str,) -> str:    from google.cloud import bigquery, aiplatform    import pandas as pd    import numpy as np    # \\u2705 Define these INSIDE the function    features = [        \\\"open\\\", \\\"high\\\", \\\"low\\\", \\\"close\\\", \\\"volume\\\", \\\"ma_7\\\", \\\"ma_30\\\",        \\\"volatility_7d\\\", \\\"RSI_14\\\", \\\"MACD\\\", \\\"MACD_signal\\\",        \\\"bb_upper\\\", \\\"bb_middle\\\", \\\"bb_lower\\\", \\\"EMA20\\\",    ]    target = \\\"daily_return\\\"    aiplatform.init(project=project_id, location=location)    endpoint = aiplatform.Endpoint(endpoint_name=endpoint_name)    query = \\\"\\\"\\\"        SELECT *        FROM `sso-gcp-dba-ctm4-pub-cc10274.sp500_dataset.sp500_features`        WHERE daily_return IS NOT NULL    \\\"\\\"\\\"    bq = bigquery.Client(project=project_id)    fresh_data = bq.query(query).to_dataframe()    # \\u2705 Now this works because features/target are defined above    fresh_data = fresh_data.dropna(subset=features + [target])    X = fresh_data[features].values    instances = X.tolist()    batch_size = 256    all_predictions = []    num_rows = len(instances)    print(f\\\"Total rows to score: {num_rows}\\\")    for start in range(0, num_rows, batch_size):        end = min(start + batch_size, num_rows)        batch_instances = instances[start:end]        print(f\\\"Scoring rows {start} to {end - 1}\\\")        resp = endpoint.predict(instances=batch_instances)        all_predictions.extend(resp.predictions)    # Convert predictions to array    predictions = np.array(all_predictions).flatten()    # Example: predicted volatility & confidence    predicted_volatility = np.abs(predictions) * 0.1    confidence_scores = []    signals = []    buy_threshold = 0.001    volatility_threshold = 0.05    confidence_threshold = 0.8    max_possible_residual = 0.1    for i, pred in enumerate(predictions):        actual = fresh_data[target].iloc[i]        residual = abs(pred - actual)        confidence_score = max(            0.5, 1 - (residual / (max_possible_residual + 0.1))        )        confidence_scores.append(confidence_score)        vol = predicted_volatility[i]        if (            pred > buy_threshold            and confidence_score > confidence_threshold            and vol < volatility_threshold        ):            signals.append(\\\"buy\\\")        else:            signals.append(\\\"sell\\\")    fresh_data[\\\"predicted_daily_return\\\"] = predictions    fresh_data[\\\"predicted_volatility\\\"] = predicted_volatility    fresh_data[\\\"confidence_score\\\"] = confidence_scores    fresh_data[\\\"signal\\\"] = signals    # Write predictions back to BigQuery    table_id = (        \\\"sso-gcp-dba-ctm4-pub-cc10274.sp500_dataset.sp500_predictions_fz\\\"    )    fresh_data[        [\\\"Name\\\", \\\"date\\\", \\\"predicted_daily_return\\\",         \\\"predicted_volatility\\\", \\\"confidence_score\\\", \\\"signal\\\"]    ].to_gbq(table_id, project_id=project_id, if_exists=\\\"append\\\")    print(\\\"Predictions uploaded to BigQuery\\\")    return \\\"Predictions uploaded\\\"\"\n          ],\n          \"image\": \"python:3.9\"\n        }\n      },\n      \"exec-train-model-op\": {\n        \"container\": {\n          \"args\": [\n            \"--executor_input\",\n            \"{{$}}\",\n            \"--function_to_execute\",\n            \"train_model_op\"\n          ],\n          \"command\": [\n            \"sh\",\n            \"-c\",\n            \"if ! [ -x \\\"$(command -v pip)\\\" ]; then    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pipfiPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'google-cloud-storage' 'scikit-learn' 'joblib' 'db-dtypes'  &&  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.14.4' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\\\"3.9\\\"' && \\\"$0\\\" \\\"$@\\\"\",\n            \"sh\",\n            \"-ec\",\n            \"program_path=$(mktemp -d)printf \\\"%s\\\" \\\"$0\\\" > \\\"$program_path/ephemeral_component.py\\\"_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \\\"$program_path/ephemeral_component.py\\\"                         \\\"$@\\\"\",\n            \"import kfpfrom kfp import dslfrom kfp.dsl import *from typing import *def train_model_op(processed_data_path: str, bucket_name: str) -> str:    from google.cloud import storage    from sklearn.linear_model import LinearRegression    from sklearn.metrics import mean_squared_error    import joblib    import os    storage_client = storage.Client()    bucket = storage_client.bucket(bucket_name)    relative = processed_data_path.replace(f\\\"gs://{bucket_name}/\\\", \\\"\\\")    bucket.blob(relative).download_to_filename(\\\"/tmp/processed_data.joblib\\\")    data = joblib.load(\\\"/tmp/processed_data.joblib\\\")    model = LinearRegression()    model.fit(data[\\\"X_train\\\"], data[\\\"y_train\\\"])    y_pred = model.predict(data[\\\"X_test\\\"])    rmse = (mean_squared_error(data[\\\"y_test\\\"], y_pred)) ** 0.5    print(\\\"Model RMSE:\\\", rmse)    os.makedirs(\\\"/tmp/models\\\", exist_ok=True)    model_path = \\\"/tmp/models/sp500_model.joblib\\\"    joblib.dump(model, model_path)    # Upload model to GCS    bucket.blob(\\\"models/sp500_model.joblib\\\").upload_from_filename(model_path)    return f\\\"gs://{bucket_name}/models/sp500_model.joblib\\\"\"\n          ],\n          \"image\": \"python:3.9\"\n        }\n      }\n    }\n  },\n  \"pipelineInfo\": {\n    \"description\": \"Pipeline to preprocess SP500 data, train model and register on Vertex AI\",\n    \"name\": \"sp500-training-pipeline\"\n  },\n  \"root\": {\n    \"dag\": {\n      \"tasks\": {\n        \"load-preprocess-op\": {\n          \"cachingOptions\": {\n            \"enableCache\": true\n          },\n          \"componentRef\": {\n            \"name\": \"comp-load-preprocess-op\"\n          },\n          \"inputs\": {\n            \"parameters\": {\n              \"bucket_name\": {\n                \"componentInputParameter\": \"bucket_name\"\n              },\n              \"project_id\": {\n                \"componentInputParameter\": \"project_id\"\n              },\n              \"query\": {\n                \"componentInputParameter\": \"query\"\n              }\n            }\n          },\n          \"taskInfo\": {\n            \"name\": \"load-preprocess-op\"\n          }\n        },\n        \"register-and-deploy-model-op\": {\n          \"cachingOptions\": {\n            \"enableCache\": true\n          },\n          \"componentRef\": {\n            \"name\": \"comp-register-and-deploy-model-op\"\n          },\n          \"dependentTasks\": [\n            \"train-model-op\"\n          ],\n          \"inputs\": {\n            \"parameters\": {\n              \"bucket_name\": {\n                \"componentInputParameter\": \"bucket_name\"\n              },\n              \"location\": {\n                \"componentInputParameter\": \"location\"\n              },\n              \"model_path\": {\n                \"taskOutputParameter\": {\n                  \"outputParameterKey\": \"Output\",\n                  \"producerTask\": \"train-model-op\"\n                }\n              },\n              \"project_id\": {\n                \"componentInputParameter\": \"project_id\"\n              }\n            }\n          },\n          \"taskInfo\": {\n            \"name\": \"register-and-deploy-model-op\"\n          }\n        },\n        \"score-model-op\": {\n          \"cachingOptions\": {\n            \"enableCache\": true\n          },\n          \"componentRef\": {\n            \"name\": \"comp-score-model-op\"\n          },\n          \"dependentTasks\": [\n            \"register-and-deploy-model-op\"\n          ],\n          \"inputs\": {\n            \"parameters\": {\n              \"bucket_name\": {\n                \"componentInputParameter\": \"bucket_name\"\n              },\n              \"endpoint_name\": {\n                \"taskOutputParameter\": {\n                  \"outputParameterKey\": \"Output\",\n                  \"producerTask\": \"register-and-deploy-model-op\"\n                }\n              },\n              \"location\": {\n                \"componentInputParameter\": \"location\"\n              },\n              \"project_id\": {\n                \"componentInputParameter\": \"project_id\"\n              }\n            }\n          },\n          \"taskInfo\": {\n            \"name\": \"score-model-op\"\n          }\n        },\n        \"train-model-op\": {\n          \"cachingOptions\": {\n            \"enableCache\": true\n          },\n          \"componentRef\": {\n            \"name\": \"comp-train-model-op\"\n          },\n          \"dependentTasks\": [\n            \"load-preprocess-op\"\n          ],\n          \"inputs\": {\n            \"parameters\": {\n              \"bucket_name\": {\n                \"componentInputParameter\": \"bucket_name\"\n              },\n              \"processed_data_path\": {\n                \"taskOutputParameter\": {\n                  \"outputParameterKey\": \"Output\",\n                  \"producerTask\": \"load-preprocess-op\"\n                }\n              }\n            }\n          },\n          \"taskInfo\": {\n            \"name\": \"train-model-op\"\n          }\n        }\n      }\n    },\n    \"inputDefinitions\": {\n      \"parameters\": {\n        \"bucket_name\": {\n          \"parameterType\": \"STRING\"\n        },\n        \"location\": {\n          \"parameterType\": \"STRING\"\n        },\n        \"project_id\": {\n          \"parameterType\": \"STRING\"\n        },\n        \"query\": {\n          \"parameterType\": \"STRING\"\n        }\n      }\n    }\n  },\n  \"schemaVersion\": \"2.1.0\",\n  \"sdkVersion\": \"kfp-2.14.4\"\n}",
      "ConnectionProfile" : "replace-with-usercode-uc-VERTEX-CP",
      "Action" : "Run A Pipeline",
      "Add Parameters" : "checked",
      "Project Name" : "sso-gcp-dba-ctm4-pub-cc10274",
      "Service Account" : "sso-gcp-dba-ctm4-pub-cc10274@appspot.gserviceaccount.com",
      "GCS Output Directory" : "gs://bmc-ctm-test",
      "Pipeline Runtime Paramaters" : "\"parameterValues\":{\"bucket_name\":\"sp500-etl\",\"location\":\"us-west2\",\"project_id\":\"sso-gcp-dba-ctm4-pub-cc10274\",\"query\":\"SELECT * FROM `sso-gcp-dba-ctm4-pub-cc10274.sp500_dataset.sp500_features` WHERE daily_return IS NOT NULL\"}%4E",
      "Description" : "This job triggers the vertex that trains the model, generate predictions and store the result in Big Query",
      "RunAs" : "replace-with-usercode-uc-VERTEX-CP",
      "When" : {
        "WeekDays" : [ "NONE" ],
        "MonthDays" : [ "ALL" ],
        "DaysRelation" : "OR"
      },
      "eventsToWaitFor" : {
        "Type" : "WaitForEvents",
        "Events" : [ {
          "Event" : "GCP_BigQuery_Job_1-TO-GCP_Vertex_AI_Job_300"
        } ]
      },
      "eventsToAdd" : {
        "Type" : "AddEvents",
        "Events" : [ {
          "Event" : "GCP_Vertex_AI_Job_300-TO-Looker_refresh"
        } ]
      },
      "eventsToDelete" : {
        "Type" : "DeleteEvents",
        "Events" : [ {
          "Event" : "GCP_BigQuery_Job_1-TO-GCP_Vertex_AI_Job_300"
        } ]
      }
    }
  }
}
