{
  {
  "Defaults": {
    "Host": "replace-with-usercode_instruqt_server",
    "CreatedBy": "replace-with-email",
    "ControlmServer": "IN01",
    "Application": "replace-with-usercode_SAP500",
    "SubApplication": "replace-with-usercode_SAP500",
    "OrderMethod": "Manual"
  },
  "replace-with-usercode-SP-500-vertex-AI" : {
    "Type" : "Folder",
    "ActiveRetentionPolicy" : "CleanEndedOK",
    "When" : {
      "RuleBasedCalendars" : {
        "Included" : [ "EVERYDAY" ],
        "EVERYDAY" : {
          "Type" : "Calendar:RuleBased",
          "When" : {
            "DaysRelation" : "OR",
            "WeekDays" : [ "NONE" ],
            "MonthDays" : [ "ALL" ]
          }
        }
      }
    },
    "BigQuery-Job" : {
      "Type" : "Job:GCP BigQuery",
      "ConnectionProfile" : "replace-with-usercode-uc-BQ-CP",
      "Project Name" : "sso-gcp-dba-ctm4-pub-cc10274",
      "Dataset Name" : "sp500_dataset",
      "Action" : "Query",
      "Run Select Query and Copy to Table" : "unchecked",
      "SQL Statement" : "CREATE OR REPLACE TABLE `sso-gcp-dba-ctm4-pub-cc10274.sp500_dataset.sp500_features` AS%4ESELECT%4EDATE(date) AS date,%4Eopen,%4Ehigh,%4Elow,%4Eclose,%4Evolume,%4EName,%4ESAFE_DIVIDE(close - LAG(close) OVER (PARTITION BY Name ORDER BY DATE(date)),LAG(close) OVER (PARTITION BY Name ORDER BY DATE(date))) AS daily_return,%4EAVG(close) OVER (PARTITION BY Name ORDER BY DATE(date) ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS ma_7,%4EAVG(close) OVER (PARTITION BY Name ORDER BY DATE(date) ROWS BETWEEN 29 PRECEDING AND CURRENT ROW) AS ma_30,%4ESTDDEV(close) OVER (PARTITION BY Name ORDER BY DATE(date) ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS volatility_7d,%4EAVG(close) OVER (PARTITION BY Name ORDER BY DATE(date) ROWS BETWEEN 13 PRECEDING AND CURRENT ROW) AS RSI_14,%4EAVG(close) OVER (PARTITION BY Name ORDER BY DATE(date) ROWS BETWEEN 11 PRECEDING AND CURRENT ROW)%4E-%4EAVG(close) OVER (PARTITION BY Name ORDER BY DATE(date) ROWS BETWEEN 25 PRECEDING AND CURRENT ROW) AS MACD,%4EAVG(close) OVER (PARTITION BY Name ORDER BY DATE(date) ROWS BETWEEN 8 PRECEDING AND CURRENT ROW) AS MACD_signal,%4EAVG(close) OVER (PARTITION BY Name ORDER BY DATE(date) ROWS BETWEEN 19 PRECEDING AND CURRENT ROW)%4E+%4E2 * STDDEV(close) OVER (PARTITION BY Name ORDER BY DATE(date) ROWS BETWEEN 19 PRECEDING AND CURRENT ROW) AS bb_upper,%4EAVG(close) OVER (PARTITION BY Name ORDER BY DATE(date) ROWS BETWEEN 19 PRECEDING AND CURRENT ROW) AS bb_middle,%4EAVG(close) OVER (PARTITION BY Name ORDER BY DATE(date) ROWS BETWEEN 19 PRECEDING AND CURRENT ROW)%4E-%4E2 * STDDEV(close) OVER (PARTITION BY Name ORDER BY DATE(date) ROWS BETWEEN 19 PRECEDING AND CURRENT ROW) AS bb_lower,%4EAVG(close) OVER (PARTITION BY Name ORDER BY DATE(date) ROWS BETWEEN 19 PRECEDING AND CURRENT ROW) AS EMA20%4EFROM `sso-gcp-dba-ctm4-pub-cc10274.sp500_dataset.sp500_clean`;%4E",
      "Copy Operation Type" : "Copy",
      "Show Load Options" : "unchecked",
      "Description" : "This job further proccessses the clean data and feature engineering",
      "RunAs" : "replace-with-usercode-uc-BQ-CP",
      "When" : {
        "WeekDays" : [ "NONE" ],
        "MonthDays" : [ "ALL" ],
        "DaysRelation" : "OR"
      },
      "eventsToAdd" : {
        "Type" : "AddEvents",
        "Events" : [ {
          "Event" : "GCP_BigQuery_Job_1-TO-GCP_Vertex_AI_Job_300"
        } ]
      }
    },
    "Looker-refresh-job" : {
      "Type" : "Job:GCP Functions",
      "ConnectionProfile" : "replace-with-usercode-uc-FUNCTIONS-CP",
      "Function Parameters" : "URL Parameters",
      "Get Logs" : "unchecked",
      "att2" : "\"filter\":\"labels.execution_id:",
      "att3" : "\"orderBy\": \"timestamp desc\",   \"pageSize\": 1000 }",
      "Project ID" : "sso-gcp-dba-ctm4-pub-cc10274",
      "Location" : "us-west2",
      "Function Name" : "looker-refresh",
      "Description" : "This job triggers the GCP function that refreshes the looker dashboard",
      "RunAs" : "replace-with-usercode-uc-FUNCTIONS-CP",
      "When" : {
        "WeekDays" : [ "NONE" ],
        "MonthDays" : [ "ALL" ],
        "DaysRelation" : "OR"
      },
      "eventsToWaitFor" : {
        "Type" : "WaitForEvents",
        "Events" : [ {
          "Event" : "GCP_Vertex_AI_Job_300-TO-Looker_refresh"
        } ]
      },
      "eventsToDelete" : {
        "Type" : "DeleteEvents",
        "Events" : [ {
          "Event" : "GCP_Vertex_AI_Job_300-TO-Looker_refresh"
        } ]
      }
    },
    "Vertex-AI-pipleine-Job" : {
      "Type" : "Job:GCP Vertex AI",
      "Pipeline Specification" : "{\r\n  \"components\": {\r\n    \"comp-load-preprocess-op\": {\r\n      \"executorLabel\": \"exec-load-preprocess-op\",\r\n      \"inputDefinitions\": {\r\n        \"parameters\": {\r\n          \"bucket_name\": {\r\n            \"parameterType\": \"STRING\"\r\n          },\r\n          \"project_id\": {\r\n            \"parameterType\": \"STRING\"\r\n          },\r\n          \"query\": {\r\n            \"parameterType\": \"STRING\"\r\n          }\r\n        }\r\n      },\r\n      \"outputDefinitions\": {\r\n        \"parameters\": {\r\n          \"Output\": {\r\n            \"parameterType\": \"STRING\"\r\n          }\r\n        }\r\n      }\r\n    },\r\n    \"comp-register-and-deploy-model-op\": {\r\n      \"executorLabel\": \"exec-register-and-deploy-model-op\",\r\n      \"inputDefinitions\": {\r\n        \"parameters\": {\r\n          \"bucket_name\": {\r\n            \"parameterType\": \"STRING\"\r\n          },\r\n          \"location\": {\r\n            \"parameterType\": \"STRING\"\r\n          },\r\n          \"model_path\": {\r\n            \"parameterType\": \"STRING\"\r\n          },\r\n          \"project_id\": {\r\n            \"parameterType\": \"STRING\"\r\n          }\r\n        }\r\n      },\r\n      \"outputDefinitions\": {\r\n        \"parameters\": {\r\n          \"Output\": {\r\n            \"parameterType\": \"STRING\"\r\n          }\r\n        }\r\n      }\r\n    },\r\n    \"comp-score-model-op\": {\r\n      \"executorLabel\": \"exec-score-model-op\",\r\n      \"inputDefinitions\": {\r\n        \"parameters\": {\r\n          \"bucket_name\": {\r\n            \"parameterType\": \"STRING\"\r\n          },\r\n          \"endpoint_name\": {\r\n            \"parameterType\": \"STRING\"\r\n          },\r\n          \"location\": {\r\n            \"parameterType\": \"STRING\"\r\n          },\r\n          \"project_id\": {\r\n            \"parameterType\": \"STRING\"\r\n          }\r\n        }\r\n      },\r\n      \"outputDefinitions\": {\r\n        \"parameters\": {\r\n          \"Output\": {\r\n            \"parameterType\": \"STRING\"\r\n          }\r\n        }\r\n      }\r\n    },\r\n    \"comp-train-model-op\": {\r\n      \"executorLabel\": \"exec-train-model-op\",\r\n      \"inputDefinitions\": {\r\n        \"parameters\": {\r\n          \"bucket_name\": {\r\n            \"parameterType\": \"STRING\"\r\n          },\r\n          \"processed_data_path\": {\r\n            \"parameterType\": \"STRING\"\r\n          }\r\n        }\r\n      },\r\n      \"outputDefinitions\": {\r\n        \"parameters\": {\r\n          \"Output\": {\r\n            \"parameterType\": \"STRING\"\r\n          }\r\n        }\r\n      }\r\n    }\r\n  },\r\n  \"deploymentSpec\": {\r\n    \"executors\": {\r\n      \"exec-load-preprocess-op\": {\r\n        \"container\": {\r\n          \"args\": [\r\n            \"--executor_input\",\r\n            \"{{$}}\",\r\n            \"--function_to_execute\",\r\n            \"load_preprocess_op\"\r\n          ],\r\n          \"command\": [\r\n            \"sh\",\r\n            \"-c\",\r\n            \"\\nif ! [ -x \\\"$(command -v pip)\\\" ]; then\\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\\nfi\\n\\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'google-cloud-bigquery' 'google-cloud-storage' 'pandas' 'scikit-learn' 'joblib' 'db-dtypes'  &&  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.14.4' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\\\"3.9\\\"' && \\\"$0\\\" \\\"$@\\\"\\n\",\r\n            \"sh\",\r\n            \"-ec\",\r\n            \"program_path=$(mktemp -d)\\n\\nprintf \\\"%s\\\" \\\"$0\\\" > \\\"$program_path/ephemeral_component.py\\\"\\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \\\"$program_path/ephemeral_component.py\\\"                         \\\"$@\\\"\\n\",\r\n            \"\\nimport kfp\\nfrom kfp import dsl\\nfrom kfp.dsl import *\\nfrom typing import *\\n\\ndef load_preprocess_op(project_id: str, query: str, bucket_name: str) -> str:\\n    from google.cloud import bigquery, storage\\n    import pandas as pd\\n    from sklearn.model_selection import train_test_split\\n    from sklearn.impute import SimpleImputer\\n    from sklearn.preprocessing import StandardScaler\\n    import joblib\\n    import os\\n\\n    # Load data\\n    bq = bigquery.Client(project=project_id)\\n    df = bq.query(query).to_dataframe()\\n\\n    features = [\\n        \\\"open\\\", \\\"high\\\", \\\"low\\\", \\\"close\\\", \\\"volume\\\", \\\"ma_7\\\", \\\"ma_30\\\",\\n        \\\"volatility_7d\\\", \\\"RSI_14\\\", \\\"MACD\\\", \\\"MACD_signal\\\",\\n        \\\"bb_upper\\\", \\\"bb_middle\\\", \\\"bb_lower\\\", \\\"EMA20\\\"\\n    ]\\n    target = \\\"daily_return\\\"\\n\\n    X = df[features]\\n    y = df[target]\\n\\n    imputer = SimpleImputer(strategy=\\\"mean\\\")\\n    X_imputed = imputer.fit_transform(X)\\n    y_imputed = imputer.fit_transform(y.values.reshape(-1, 1)).flatten()\\n\\n    scaler = StandardScaler()\\n    X_scaled = scaler.fit_transform(X_imputed)\\n\\n    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_imputed, test_size=0.2)\\n\\n    os.makedirs(\\\"/tmp/data\\\", exist_ok=True)\\n    processed_path = \\\"/tmp/data/processed_data.joblib\\\"\\n\\n    joblib.dump(\\n        {\\n            \\\"X_train\\\": X_train,\\n            \\\"X_test\\\": X_test,\\n            \\\"y_train\\\": y_train,\\n            \\\"y_test\\\": y_test,\\n        },\\n        processed_path,\\n    )\\n\\n    storage_client = storage.Client(project=project_id)\\n    bucket = storage_client.bucket(bucket_name)\\n    blob = bucket.blob(\\\"pipeline_artifacts/processed_data.joblib\\\")\\n    blob.upload_from_filename(processed_path)\\n\\n    return f\\\"gs://{bucket_name}/pipeline_artifacts/processed_data.joblib\\\"\\n\\n\"\r\n          ],\r\n          \"image\": \"python:3.9\"\r\n        }\r\n      },\r\n      \"exec-register-and-deploy-model-op\": {\r\n        \"container\": {\r\n          \"args\": [\r\n            \"--executor_input\",\r\n            \"{{$}}\",\r\n            \"--function_to_execute\",\r\n            \"register_and_deploy_model_op\"\r\n          ],\r\n          \"command\": [\r\n            \"sh\",\r\n            \"-c\",\r\n            \"\\nif ! [ -x \\\"$(command -v pip)\\\" ]; then\\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\\nfi\\n\\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'google-cloud-aiplatform>=1.38.0'  &&  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.14.4' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\\\"3.9\\\"' && \\\"$0\\\" \\\"$@\\\"\\n\",\r\n            \"sh\",\r\n            \"-ec\",\r\n            \"program_path=$(mktemp -d)\\n\\nprintf \\\"%s\\\" \\\"$0\\\" > \\\"$program_path/ephemeral_component.py\\\"\\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \\\"$program_path/ephemeral_component.py\\\"                         \\\"$@\\\"\\n\",\r\n            \"\\nimport kfp\\nfrom kfp import dsl\\nfrom kfp.dsl import *\\nfrom typing import *\\n\\ndef register_and_deploy_model_op(model_path: str, project_id: str, location: str, bucket_name: str) -> str:\\n    from google.cloud import aiplatform as aiplatform\\n    from google.cloud import bigquery, storage\\n\\n    # Initialize Vertex AI\\n    aiplatform.init(project=project_id, location=location)\\n\\n    # Ensure the model is in GCS, upload it if needed\\n    storage_client = storage.Client(project=project_id)\\n    bucket = storage_client.bucket(bucket_name)\\n\\n    # Check if the model file exists in GCS, else upload it\\n    local_model_path = \\\"/tmp/model/sp500_model.joblib\\\"\\n    blob = bucket.blob(\\\"models/sp500_model.joblib\\\")\\n\\n    if not blob.exists():\\n        print(\\\"Uploading model to GCS...\\\")\\n        blob.upload_from_filename(local_model_path)\\n    else:\\n        print(\\\"Model already exists in GCS.\\\")\\n\\n    # Now use the GCS path for artifact_uri\\n    model_uri = f\\\"gs://{bucket_name}/models/sp500_model.joblib\\\"\\n\\n    # Register the model in Vertex AI\\n    model = aiplatform.Model.upload(\\n        display_name=\\\"sp500-linear-regression-model-v1\\\",\\n        artifact_uri=\\\"gs://sp500-etl/models/\\\",\\n        serving_container_image_uri=\\\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-3:latest\\\",\\n        description=\\\"Linear Regression model for predicting SP500 daily returns\\\"\\n    )\\n\\n    print(f\\\"Model registered successfully. Model resource name: {model.resource_name}\\\")\\n\\n    # Deploy the model to an endpoint\\n    endpoint = model.deploy(\\n        deployed_model_display_name=\\\"sp500-endpoint\\\",\\n        machine_type=\\\"n1-standard-2\\\"\\n    )\\n\\n    print(f\\\"Model deployed successfully to endpoint: {endpoint.resource_name}\\\")\\n\\n    return endpoint.resource_name  # Return the endpoint resource name for use in the next step\\n\\n\"\r\n          ],\r\n          \"image\": \"python:3.10\"\r\n        }\r\n      },\r\n      \"exec-score-model-op\": {\r\n        \"container\": {\r\n          \"args\": [\r\n            \"--executor_input\",\r\n            \"{{$}}\",\r\n            \"--function_to_execute\",\r\n            \"score_model_op\"\r\n          ],\r\n          \"command\": [\r\n            \"sh\",\r\n            \"-c\",\r\n            \"\\nif ! [ -x \\\"$(command -v pip)\\\" ]; then\\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\\nfi\\n\\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'google-cloud-bigquery' 'google-cloud-aiplatform' 'pandas' 'numpy' 'db-dtypes' 'pandas-gbq' 'scikit-learn'  &&  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.14.4' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\\\"3.9\\\"' && \\\"$0\\\" \\\"$@\\\"\\n\",\r\n            \"sh\",\r\n            \"-ec\",\r\n            \"program_path=$(mktemp -d)\\n\\nprintf \\\"%s\\\" \\\"$0\\\" > \\\"$program_path/ephemeral_component.py\\\"\\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \\\"$program_path/ephemeral_component.py\\\"                         \\\"$@\\\"\\n\",\r\n            \"\\nimport kfp\\nfrom kfp import dsl\\nfrom kfp.dsl import *\\nfrom typing import *\\n\\ndef score_model_op(\\n    endpoint_name: str,\\n    project_id: str,\\n    location: str,\\n    bucket_name: str,\\n) -> str:\\n    from google.cloud import bigquery, aiplatform\\n    import pandas as pd\\n    import numpy as np\\n\\n    # \\u2705 Define these INSIDE the function\\n    features = [\\n        \\\"open\\\", \\\"high\\\", \\\"low\\\", \\\"close\\\", \\\"volume\\\", \\\"ma_7\\\", \\\"ma_30\\\",\\n        \\\"volatility_7d\\\", \\\"RSI_14\\\", \\\"MACD\\\", \\\"MACD_signal\\\",\\n        \\\"bb_upper\\\", \\\"bb_middle\\\", \\\"bb_lower\\\", \\\"EMA20\\\",\\n    ]\\n    target = \\\"daily_return\\\"\\n\\n    aiplatform.init(project=project_id, location=location)\\n    endpoint = aiplatform.Endpoint(endpoint_name=endpoint_name)\\n\\n    query = \\\"\\\"\\\"\\n        SELECT *\\n        FROM `sso-gcp-dba-ctm4-pub-cc10274.sp500_dataset.sp500_features`\\n        WHERE daily_return IS NOT NULL\\n    \\\"\\\"\\\"\\n    bq = bigquery.Client(project=project_id)\\n    fresh_data = bq.query(query).to_dataframe()\\n\\n    # \\u2705 Now this works because features/target are defined above\\n    fresh_data = fresh_data.dropna(subset=features + [target])\\n\\n    X = fresh_data[features].values\\n    instances = X.tolist()\\n\\n\\n    batch_size = 256\\n    all_predictions = []\\n    num_rows = len(instances)\\n    print(f\\\"Total rows to score: {num_rows}\\\")\\n\\n    for start in range(0, num_rows, batch_size):\\n        end = min(start + batch_size, num_rows)\\n        batch_instances = instances[start:end]\\n        print(f\\\"Scoring rows {start} to {end - 1}\\\")\\n\\n        resp = endpoint.predict(instances=batch_instances)\\n        all_predictions.extend(resp.predictions)\\n\\n    # Convert predictions to array\\n    predictions = np.array(all_predictions).flatten()\\n\\n    # Example: predicted volatility & confidence\\n    predicted_volatility = np.abs(predictions) * 0.1\\n\\n    confidence_scores = []\\n    signals = []\\n    buy_threshold = 0.001\\n    volatility_threshold = 0.05\\n    confidence_threshold = 0.8\\n    max_possible_residual = 0.1\\n\\n    for i, pred in enumerate(predictions):\\n        actual = fresh_data[target].iloc[i]\\n        residual = abs(pred - actual)\\n\\n        confidence_score = max(\\n            0.5, 1 - (residual / (max_possible_residual + 0.1))\\n        )\\n        confidence_scores.append(confidence_score)\\n\\n        vol = predicted_volatility[i]\\n\\n        if (\\n            pred > buy_threshold\\n            and confidence_score > confidence_threshold\\n            and vol < volatility_threshold\\n        ):\\n            signals.append(\\\"buy\\\")\\n        else:\\n            signals.append(\\\"sell\\\")\\n\\n    fresh_data[\\\"predicted_daily_return\\\"] = predictions\\n    fresh_data[\\\"predicted_volatility\\\"] = predicted_volatility\\n    fresh_data[\\\"confidence_score\\\"] = confidence_scores\\n    fresh_data[\\\"signal\\\"] = signals\\n\\n    # Write predictions back to BigQuery\\n    table_id = (\\n        \\\"sso-gcp-dba-ctm4-pub-cc10274.sp500_dataset.sp500_predictions_fz\\\"\\n    )\\n\\n    fresh_data[\\n        [\\\"Name\\\", \\\"date\\\", \\\"predicted_daily_return\\\",\\n         \\\"predicted_volatility\\\", \\\"confidence_score\\\", \\\"signal\\\"]\\n    ].to_gbq(table_id, project_id=project_id, if_exists=\\\"append\\\")\\n\\n    print(\\\"Predictions uploaded to BigQuery\\\")\\n    return \\\"Predictions uploaded\\\"\\n\\n\"\r\n          ],\r\n          \"image\": \"python:3.9\"\r\n        }\r\n      },\r\n      \"exec-train-model-op\": {\r\n        \"container\": {\r\n          \"args\": [\r\n            \"--executor_input\",\r\n            \"{{$}}\",\r\n            \"--function_to_execute\",\r\n            \"train_model_op\"\r\n          ],\r\n          \"command\": [\r\n            \"sh\",\r\n            \"-c\",\r\n            \"\\nif ! [ -x \\\"$(command -v pip)\\\" ]; then\\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\\nfi\\n\\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'google-cloud-storage' 'scikit-learn' 'joblib' 'db-dtypes'  &&  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.14.4' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\\\"3.9\\\"' && \\\"$0\\\" \\\"$@\\\"\\n\",\r\n            \"sh\",\r\n            \"-ec\",\r\n            \"program_path=$(mktemp -d)\\n\\nprintf \\\"%s\\\" \\\"$0\\\" > \\\"$program_path/ephemeral_component.py\\\"\\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \\\"$program_path/ephemeral_component.py\\\"                         \\\"$@\\\"\\n\",\r\n            \"\\nimport kfp\\nfrom kfp import dsl\\nfrom kfp.dsl import *\\nfrom typing import *\\n\\ndef train_model_op(processed_data_path: str, bucket_name: str) -> str:\\n    from google.cloud import storage\\n    from sklearn.linear_model import LinearRegression\\n    from sklearn.metrics import mean_squared_error\\n    import joblib\\n    import os\\n\\n    storage_client = storage.Client()\\n    bucket = storage_client.bucket(bucket_name)\\n    relative = processed_data_path.replace(f\\\"gs://{bucket_name}/\\\", \\\"\\\")\\n    bucket.blob(relative).download_to_filename(\\\"/tmp/processed_data.joblib\\\")\\n\\n    data = joblib.load(\\\"/tmp/processed_data.joblib\\\")\\n\\n    model = LinearRegression()\\n    model.fit(data[\\\"X_train\\\"], data[\\\"y_train\\\"])\\n\\n    y_pred = model.predict(data[\\\"X_test\\\"])\\n    rmse = (mean_squared_error(data[\\\"y_test\\\"], y_pred)) ** 0.5\\n    print(\\\"Model RMSE:\\\", rmse)\\n\\n    os.makedirs(\\\"/tmp/models\\\", exist_ok=True)\\n    model_path = \\\"/tmp/models/sp500_model.joblib\\\"\\n    joblib.dump(model, model_path)\\n\\n    # Upload model to GCS\\n    bucket.blob(\\\"models/sp500_model.joblib\\\").upload_from_filename(model_path)\\n\\n    return f\\\"gs://{bucket_name}/models/sp500_model.joblib\\\"\\n\\n\"\r\n          ],\r\n          \"image\": \"python:3.9\"\r\n        }\r\n      }\r\n    }\r\n  },\r\n  \"pipelineInfo\": {\r\n    \"description\": \"Pipeline to preprocess SP500 data, train model and register on Vertex AI\",\r\n    \"name\": \"sp500-training-pipeline\"\r\n  },\r\n  \"root\": {\r\n    \"dag\": {\r\n      \"tasks\": {\r\n        \"load-preprocess-op\": {\r\n          \"cachingOptions\": {\r\n            \"enableCache\": true\r\n          },\r\n          \"componentRef\": {\r\n            \"name\": \"comp-load-preprocess-op\"\r\n          },\r\n          \"inputs\": {\r\n            \"parameters\": {\r\n              \"bucket_name\": {\r\n                \"componentInputParameter\": \"bucket_name\"\r\n              },\r\n              \"project_id\": {\r\n                \"componentInputParameter\": \"project_id\"\r\n              },\r\n              \"query\": {\r\n                \"componentInputParameter\": \"query\"\r\n              }\r\n            }\r\n          },\r\n          \"taskInfo\": {\r\n            \"name\": \"load-preprocess-op\"\r\n          }\r\n        },\r\n        \"register-and-deploy-model-op\": {\r\n          \"cachingOptions\": {\r\n            \"enableCache\": true\r\n          },\r\n          \"componentRef\": {\r\n            \"name\": \"comp-register-and-deploy-model-op\"\r\n          },\r\n          \"dependentTasks\": [\r\n            \"train-model-op\"\r\n          ],\r\n          \"inputs\": {\r\n            \"parameters\": {\r\n              \"bucket_name\": {\r\n                \"componentInputParameter\": \"bucket_name\"\r\n              },\r\n              \"location\": {\r\n                \"componentInputParameter\": \"location\"\r\n              },\r\n              \"model_path\": {\r\n                \"taskOutputParameter\": {\r\n                  \"outputParameterKey\": \"Output\",\r\n                  \"producerTask\": \"train-model-op\"\r\n                }\r\n              },\r\n              \"project_id\": {\r\n                \"componentInputParameter\": \"project_id\"\r\n              }\r\n            }\r\n          },\r\n          \"taskInfo\": {\r\n            \"name\": \"register-and-deploy-model-op\"\r\n          }\r\n        },\r\n        \"score-model-op\": {\r\n          \"cachingOptions\": {\r\n            \"enableCache\": true\r\n          },\r\n          \"componentRef\": {\r\n            \"name\": \"comp-score-model-op\"\r\n          },\r\n          \"dependentTasks\": [\r\n            \"register-and-deploy-model-op\"\r\n          ],\r\n          \"inputs\": {\r\n            \"parameters\": {\r\n              \"bucket_name\": {\r\n                \"componentInputParameter\": \"bucket_name\"\r\n              },\r\n              \"endpoint_name\": {\r\n                \"taskOutputParameter\": {\r\n                  \"outputParameterKey\": \"Output\",\r\n                  \"producerTask\": \"register-and-deploy-model-op\"\r\n                }\r\n              },\r\n              \"location\": {\r\n                \"componentInputParameter\": \"location\"\r\n              },\r\n              \"project_id\": {\r\n                \"componentInputParameter\": \"project_id\"\r\n              }\r\n            }\r\n          },\r\n          \"taskInfo\": {\r\n            \"name\": \"score-model-op\"\r\n          }\r\n        },\r\n        \"train-model-op\": {\r\n          \"cachingOptions\": {\r\n            \"enableCache\": true\r\n          },\r\n          \"componentRef\": {\r\n            \"name\": \"comp-train-model-op\"\r\n          },\r\n          \"dependentTasks\": [\r\n            \"load-preprocess-op\"\r\n          ],\r\n          \"inputs\": {\r\n            \"parameters\": {\r\n              \"bucket_name\": {\r\n                \"componentInputParameter\": \"bucket_name\"\r\n              },\r\n              \"processed_data_path\": {\r\n                \"taskOutputParameter\": {\r\n                  \"outputParameterKey\": \"Output\",\r\n                  \"producerTask\": \"load-preprocess-op\"\r\n                }\r\n              }\r\n            }\r\n          },\r\n          \"taskInfo\": {\r\n            \"name\": \"train-model-op\"\r\n          }\r\n        }\r\n      }\r\n    },\r\n    \"inputDefinitions\": {\r\n      \"parameters\": {\r\n        \"bucket_name\": {\r\n          \"parameterType\": \"STRING\"\r\n        },\r\n        \"location\": {\r\n          \"parameterType\": \"STRING\"\r\n        },\r\n        \"project_id\": {\r\n          \"parameterType\": \"STRING\"\r\n        },\r\n        \"query\": {\r\n          \"parameterType\": \"STRING\"\r\n        }\r\n      }\r\n    }\r\n  },\r\n  \"schemaVersion\": \"2.1.0\",\r\n  \"sdkVersion\": \"kfp-2.14.4\"\r\n}",
      "ConnectionProfile" : "replace-with-usercode-uc-VERTEX-CP",
      "Action" : "Run A Pipeline",
      "Add Parameters" : "checked",
      "Project Name" : "sso-gcp-dba-ctm4-pub-cc10274",
      "Service Account" : "sso-gcp-dba-ctm4-pub-cc10274@appspot.gserviceaccount.com",
      "GCS Output Directory" : "gs://bmc-ctm-test",
      "Pipeline Runtime Paramaters" : "\"parameterValues\":{\"bucket_name\":\"sp500-etl\",\"location\":\"us-west2\",\"project_id\":\"sso-gcp-dba-ctm4-pub-cc10274\",\"query\":\"SELECT * FROM `sso-gcp-dba-ctm4-pub-cc10274.sp500_dataset.sp500_features` WHERE daily_return IS NOT NULL\"}%4E",
      "Description" : "This job triggers the vertex that trains the model, generate predictions and store the result in Big Query",
      "RunAs" : "replace-with-usercode-uc-VERTEX-CP",
      "When" : {
        "WeekDays" : [ "NONE" ],
        "MonthDays" : [ "ALL" ],
        "DaysRelation" : "OR"
      },
      "eventsToWaitFor" : {
        "Type" : "WaitForEvents",
        "Events" : [ {
          "Event" : "GCP_BigQuery_Job_1-TO-GCP_Vertex_AI_Job_300"
        } ]
      },
      "eventsToAdd" : {
        "Type" : "AddEvents",
        "Events" : [ {
          "Event" : "GCP_Vertex_AI_Job_300-TO-Looker_refresh"
        } ]
      },
      "eventsToDelete" : {
        "Type" : "DeleteEvents",
        "Events" : [ {
          "Event" : "GCP_BigQuery_Job_1-TO-GCP_Vertex_AI_Job_300"
        } ]
      }
    }
  }
}
